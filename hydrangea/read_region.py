import numpy as np
import h5py as h5
import time
import os

from functools import reduce
from operator import mul
from astropy.cosmology import Planck13
import astropy.units as u

import hydrangea.crossref as hxr
import hydrangea.hdf5 as hdf5
from hydrangea.split_file import SplitFile
from hydrangea.reader_base import ReaderBase

from pdb import set_trace


# ----------- READ_REGION CLASS -------------------

class ReadRegion(ReaderBase):
    """
    Set up a region for efficient reading of data from snapshot files.
    
    This class can be called with several parameters to allow easy setup
    of commonly encountered selection regions (sphere, cube, box). Internally,
    the particle map generated by MapMaker is then read and processed into a 
    (typically small) number of segments to read in.

    Note
    ----
    Setting up very large regions (>~ 20 Mpc) is not very efficient.
    Therefore, when the code detects that > 1M cells would have to be 
    checked and potentially loaded, the region setup is abandoned and the
    entire particle catalogue will be read in. Full reading is also enforced
    when the sub-selection contains more than 40% of the full catalogue.
    """

    def __init__(self, file_name, part_type, coordinates, shape=None,
                 anchor=None, verbose=False, silent=False, exact=False,
                 astro=True, map_file=None, periodic=False, load_full=False,
                 join_threshold=100, bridge_threshold=100, bridge_gap=0.5):
        """
        Parameters
        ----------
        file_name : string
            The path of the file containing the data to read. If the data 
            is spread over multiple files, it should point to the first 
            of them.
        part_type : int
            The (numerical) particle type to read 
            (0=gas, 1=dm, 4=stars, 5=bh)
        coordinates : array (float)
            The coordinates, in code units, that specify the region to read 
            from. This depends on the selected shape (below):
                Sphere (default) --> [x_cen, y_cen, z_cen, r] (r = radius)
                Cube --> [x_cen, y_cen, z_cen, a/2] (a = side length)
                Box --> [x_cen, y_cen_z_cen, ax/2, ay/2, az/2] 
                     (ax, ay, az = side lengths along x, y, z axes)
            Note that the cube and box coordinates are interpreted 
            differently if the anchor parameter (below) is set to 'bottom'.
        shape : string, optional
            The shape of the region to read from. Valid options are 'sphere',
            'cube', or 'box' (capitalized versions also accepted). If None
            (default), a sphere is used.
        anchor : string, optional
            If 'bottom', the 'centre' coordinates for cube and box shapes are
            interpreted instead as the minimum x, y, z coordinates, and the 
            length(s) as full side length. If 'centre' or None (default), they 
            specify the centre instead, as described above. This parameter 
            has no effect when a sphere is selected as shape.
        verbose : bool, optional
            If True, more output is printed (default: False)
        silent : bool, optional
            If True, no output is printed (default: False)
        exact : bool, optional
            Only load particles lying exactly within the specified region,
            at extra cost (internally reads particle positions). If False
            (default), typically also some particles slightly outside the 
            selected region are loaded.
        astro : bool, optional
            Interpret the supplied coordinates in proper Mpc instead of code 
            units (cMpc/h). Default: True.
        map_file : string, optional
            Location of the particle map file to use. If None (default),
            it is assumed to be 'ParticleMap.hdf5' in the same directory 
            as file_name.
        periodic : bool, optional
            Assume that the simulation volume is periodic and completely
            tiled with particle map cells (default: False).
        load_full : bool, optional
            Ignore specified region and load full particle catalogue
            (default: False).
        """ 

        stime = time.clock()

        # Store 'simple' parameters in internal variables
        self.file_name = file_name
        self.pt_num = part_type
        self.base_group = "PartType{:d}" .format(part_type)
        self.coordinates = np.array(coordinates, dtype = float)
        self.verbose = verbose
        self.silent = silent
        self.exact = exact
        self.load_full = load_full
        self.periodic = periodic
        
        # Deal with different shape inputs
        if shape is None:
            self.shape = 'sphere'
        elif shape.lower() in ['sphere', 'box', 'cube']:
            self.shape = shape.lower()
        else:
            print("Your shape '{:s}' is not understood. Sorry."
                  .format(shape))
            set_trace()

        if anchor is None:
            self.anchor = 'centre'
        else:
            self.anchor = anchor
            
        # Set up map file, and check whether it exists
        if map_file is None:
            map_file = os.path.dirname(file_name) + '/ParticleMap.hdf5'
        if not os.path.exists(map_file):
            if not silent:
                print("Could not find particle map -- load everything.")
            self.load_full = True
            
        # If we need to load everything, we can quit now
        if self.load_full:
            return

        # Convert input coordinates to code, if necessary
        if astro:
            conv_astro = self.get_astro_conv('Coordinates')
            self.coordinates /= conv_astro
                        
        # Do the actual work of finding segments to be loaded
        self._setup_region(map_file, join_threshold, bridge_threshold,
                           bridge_gap)

        # If we'd have to load too many cells, just load everything
        if self.load_full:
            return
                
        if not self.silent:
            print("Region setup took {:.3f} sec."
                  .format(time.clock()-stime))
            print("Selection region contains {:d} cells, {:d} segments, "
                  "{:d} particles, {:d} files"
                  .format(self.num_cells, self.num_segments,
                          self.num_particles, len(np.unique(self.files))))
        if verbose:
            print("  (selected files:", np.unique(self.files), ")")

        # If we'd have to load almost all particles, load everything
        if self.num_particles >= self.num_part_total * 0.75:
            self.load_full = True
            if not self.silent:
                print("Need to load {:.1f}% of total particle catalogue\n"
                      "  ==> faster to load everything instead"
                      .format(self.num_particles/self.num_part_total * 100))
                
    # -----------------------------------------------------------------------
        
    def read_data(self, dataset_name, astro=True, verbose=None,
                  return_conv=False, exact=None, file_name=None,
                  pt_name=None, single_file=False, silent=None):
        """
        Read a specified dataset for a previously set up region.
        
        Parameters
        ----------
        dataset_name : string
            The dataset to read from, including groups where appropriate. 
            The leading 'PartType[x]' must however *not* be included!

        astro : bool, optional 
            Convert values to proper astronomical units (default: True)
        verbose : bool, optional 
            Enable additional log messages (default: class init value)
        return_conv : bool, optional 
            Return a list of [data, conv_astro, aexp], where conv_astro 
            is the conversion factor internal-->astro (default: False).
        exact : bool, optional
            Only return data for particles lying in the exact specified 
            selection region. Defaults to the class init value.
        file_name : string, optional 
            Specifies an alternative path to read data from. This is useful 
            for reading data from ancillary catalogues. By default (None), 
            the file name used to set up the class instance is used.
        pt_name : string, optional
            Specifies an alternative particle-type group name. By default 
            (None), 'PartType[x]' is used.
        single_file : bool, optional
            Assume that data resides in a single file (default: False).
            This is used only for ancillary catalogues.
        silent : bool, optional
            Suppress all output (default: class init value).

        Returns
        -------
        data : array
            The data read for the particles in the selected region.
        conv_astro : float
            Code-to-astronomical conversion factor; only returned if 
            return_conv is True.
        aexp : float
            The expansion factor at the time of the particle output; only
            returned if return_conv is True.
        """                    

        stime = time.time()

        # Set up default values for non-bool optional parameters
        if file_name is None:
            file_name = self.file_name
        if pt_name is None:
            pt_name = self.base_group
        if exact is None:
            exact = self.exact
        if silent is None:
            silent = self.silent
        if verbose is None:
            verbose = self.verbose

        if exact and not self.exact:
            print("Exact region loading not enabled for this instance. "
                  "Ignoring request for exact particle loading.")
            exact = False

        full_dataset_name = pt_name + '/' + dataset_name
            
        # Deal with special case of loading the full particle catalogue 
        if self.load_full:
            if single_file:
                data_full = yb.read_hdf5(file_name, full_dataset_name)

                if astro or return_conv:
                    aexp = yb.read_hdf5_attribute(file_name, 'Header',
                                                  "ExpansionFactor")
                    conv_astro = self.get_astro_conv(dataset_name)

                    if (astro
                        and np.issubdtype(data_full.dtype, np.floating)
                        and conv_astro is not None):
                        data_full *= conv_astro    
                    if return_conv:
                        return [data_full, conv_astro, aexp]
                    else:
                        return data_full
            else:
                return SplitFile(file_name, pt_name).read_data(
                    dataset_name, astro=astro, return_conv=return_conv)

        # Deal with pathological case of no data to load:
        if self.num_segments == 0:
            if return_conv:
                return [np.zeros(0, dtype = int), None, None]
            else:
                return np.zeros(0, dtype = int)
                
        # -------------------------------------------------------------
        # Rest is for 'default' situation of reading via segment list.
        # -------------------------------------------------------------

        # Initialize current offset in output array 
        write_offset = 0     

        if single_file:
            f = h5.File(file_name, 'r')
        else:
            unique_files = np.unique(self.files)

            # To avoid duplicate work, open all files now and create a
            # reverse list to find the handles later
            rev_files = yb.create_reverse_list(unique_files)
            file_list = []
            for ifile in unique_files:
                curr_file_name = self._swap_file_name(file_name, ifile)
                file_list.append(h5.File(curr_file_name, 'r'))
            f = file_list[0]
                
        # Set up output before we iterate:
        dSet = f[full_dataset_name]
        full_shape = list(dSet.shape)      # Cannot assign to tuple ==> list
        full_shape[0] = self.num_particles
        data_full = np.empty(full_shape, dSet.dtype)
            
        # Get astronomical conversion factor
        if astro or return_conv:
            conv_astro = self.get_astro_conv(dataset_name)

        if verbose:
            print("Pre-reading '{:s}' took {:.3f} sec."
                  .format(dataSetName, time.time() - stime))
            
        # Read individual segments into correct location of output array 
        for iiseg in range(self.numSegments):
            ifile = self.files[iiseg]
            if verbose:
                print("Segment {:d}, file {:d}"
                      .format(iiseg, ifile), end="")

            if not single_file:
                # Retrieve correct file handle
                handle_index = rev_files[ifile]
                if handle_index < 0:
                    print("Index inconsistency: {:d}..."
                          .format(handle_index))
                    set_trace()
                f = file_list[handle_index]

            # Load data set, checking that it actually exists
            try:
                dSet = f[full_dataset_name]
            except:
                print("Could not load data set, please investigate.")
                set_trace()
            
            # Find out from where and to where we should read
            read_offset = self.offsets[iiseg]
            read_end = self.offsets[iiseg] + self.lengths[iiseg]
            write_end = write_offset + self.lengths[iiseg]
            if verbose:
                print(" [{:d} --> {:d}]" .format(read_offset, read_end))
            
            # If we read from a single file, but the (main) particle data
            # are split, need to adjust location to read from in this file
            if single_file:
                ifile_offset = self.file_offsets[ifile]
                read_offset += ifile_offset
                read_end += ifile_offset

            if read_end-read_offset != write_end-write_offset:
                print("Inconsistency...")
                set_trace()
                
            # Actually read the data
            # (no appreciable speed difference between following two lines)
            #data_full[writeOffset:writeEnd] = dSet[readOffset:readEnd]
            dSet.read_direct(data_full, np.s_[read_offset:read_end, ...],
                             np.s_[write_offset:write_end, ...])

            # Update write offset
            write_offset += self.lengths[iiseg]

        # Close files...!
        if singleFile:
            f.close()
        else:
            for f in file_list:
                f.close()
            
        # ------ Done with main bit, final adjustments if needed -----
            
        # Limit selection if 'exact' is set:
        if exact:
            data_full = data_full[self.ind_sel, ...]

        if astro and conv_astro is not None:
            data_full *= conv_astro    

        if not silent:
            print("Reading '{:s}' took {:.3f} sec."
                  .format(dataset_name, time.time() - stime))

        if return_conv:
            return data_full, conv_astro, aexp
        else:
            return data_full                    

    # `````````````````` End of read_data() ''''''''''''''''''''''''''''

    def total_in_region(self, dataset_name, average=False,
                        weight_quant=None, astro=False):
        """
        Convenience function to compute the total or average of 'quantity' 

        Parameters
        ----------
        dataset_name : string
            The (full) name of the data set to process, including potential
            groups that contain it (but not the base group).
        average : bool, optional
            Compute the average of particles instead of the sum (default).
        weight_quant : string, optional
            The (full) name of a data set to use as weights. If None (default),
            no weighting is performed. Supplying a weight_quant implicitly
            also sets average=True. It is the user's responsibility to 
            ensure that the weights do not sum to zero.
        astro : bool, optional
            If True, both the target and (potentially) weight quantities are
            converted to astronomical units. Default: False.
        
        Returns
        -------
        sum : array
            The sum or average over all particles in the selection region.
        """

        # Healthy sized warning message if region is poorly defined
        if self.exact is False:
            print("")
            print("**********************************************************")
            print("********************   WARNING ***************************")
            print("**********************************************************")
            print("")
            print("You have not set the 'exact' switch when establising this "
                  "region. Be aware that the reported total may include a "
                  "contribution from particles outside the target region. "
                  "Proceed with caution...")            
            print("")
            print("**********************************************************")
            print("")
            
        data = self.read_data(dataset_name, astro = astro)

        if weight_quant is None:
            if average:
                return np.mean(data, axis = 0)
            else:
                return np.sum(data, axis = 0)
        else:
            weights = self.read_data(weight_quant, astro = astro)
            return np.average(data, weights = weights, axis = 0) 
            
    # --------------------------------------------------------------
    # ----------- INTERNAL-ONLY FUNCTIONS BELOW --------------------
    # --------------------------------------------------------------

    def _setup_region(self, map_file, join_threshold, bridge_threshold,
                      bridge_gap):
        """Set up the reading region with data from the particle map.
        
        Parameters
        ----------
        map_file : str
            The name of the particle map file.
        join_threshold : float
            ***TO BE ADDED***
        bridge_threshold : float
            ***TO BE ADDED***
        bridge_gap : float
            ***TO BE ADDED***

        The result is stored in the attributes 
        num_cells, num_segments, num_particles, files, offsets, lengths

        """
         
        # Select rectangular region of simulation to be loaded
        box = self._make_selection_box()
        if self.verbose:
            print("Selection box is [({:.3f} --> {:.3f}), "
                  "({:.3f} --> {:.3f}),\n" 
                  "                  ({:.3f} --> {:.3f})]"
                  .format(*box[0, :], *box[1, :], *box[2, :]))

        # Identify cells intersecting the region of interest
        #     'cellOffsets' is the first cell per dimension
        #     'cellLength' is the number of cells per dimension
        cellOffsets, cellLength = self._identify_relevant_cells(box, f)
        self.numCells = reduce(mul, cellLength, 1)
        if not self.silent:
            print("Checking {:d} cells..." .format(self.numCells))  
        
        # No cells --> nothing to do --> done
        if self.num_cells == 0:
            self.num_segments = 0
            self.num_particles = 0
            self.files = np.zeros(0, dtype = int)
            return

        # Too many cells --> too much to do --> done
        if self.numCells > 1e6:
            self.load_full = True
            return

        # The heavy lifting: find file index, offset, and length for
        # all potentially relevant segments
        self.files, self.offsets, self.lengths = self._find_segments(
            map_file, cellOffsets, cellLength)

        # If there are many segments, try to combine them where possible
        if len(self.files) > joinThreshold:
            self._reduce_segments()

            # Second, more aggressive join: bridge small gaps
            # between segments
            if len(self.files) > bridgeThreshold:
                self._reduce_segments(rel_gap = 1+bridgeGap)

        self.numParticles = np.sum(self.lengths)
        self.numParticlesExact = self.numParticles
                    
        if self.exact:
            self._find_exact_region()
    
    def _find_exact_region(self):
        """Find particles lying exactly in the selection region"""

        # Need to explicitly set 'exact = False' here, because we have not 
        # yet set up exact loading (we are doing it right now!)
        # (and astro=False because self.coordinates is in code units here)
        pt_coords = self.read_data("Coordinates", exact=False, astro=False)

        anchor = self.coordinates[:3]
        relPos = pt_coords - anchor[None, :]  

        if self.shape == 'sphere':
            relRad = np.linalg.norm(relPos, axis=1)
            self.ind_sel = np.nonzero(relRad <= self.coordinates[3])[0]
        elif self.shape == 'cube':
            if self.anchor == 'bottom':
                self.ind_sel = np.nonzero(
                    (np.min(relPos, axis = 1) >= 0) &
                    (np.max(relPos, axis = 1) <= self.coordinates[3]))[0]
            else:
                self.ind_sel = np.nonzero(
                    np.max(np.abs(relPos), axis=1) <= self.coordinates[3])[0]
                    
        elif self.shape == 'box':
            length = self.coordinates[3:]
            if self.anchor == 'bottom':
                self.ind_sel = np.nonzero(
                    (np.min(relPos, axis = 1) >= 0) &
                    (np.max(relPos - length[None, :]) <= 0))[0]
            else:
                set_trace()
                self.ind_sel = np.nonzero(
                    np.max(np.abs(relPos) - length[None, :], axis=1) <= 0)[0]
        else:
            print("Invalid shape encountered: '{:s}'." .format(self.shape))
            set_trace()
            
        self.numParticlesExact = len(self.ind_sel)        
        
    def _make_selection_box(self):
        """
        Find the box enclosing the selection region.
        
        For the moment, *all* particles in this region will be loaded. 
        In future, we may do something more fancy that takes the actual 
        selection shape into account (`may' here includes `may not').

        This function returns a 3x2 array of 6 coordinates specifying the box:
        [[x-, y-, z-], [x+, y+, z+]]
        """

        coords = np.array(self.coordinates)
        box = np.zeros((3, 2))
        
        if self.shape == "sphere":
            if len(coords) != 4:
                print("A sphere needs four coordinates: "
                      "its centre (3), and radius (1)")
                set_trace()
            box[:, 0] = coords[:3] - coords[3]
            box[:, 1] = coords[:3] + coords[3]
                   
        elif self.shape == "cube":
            if len(coords) != 4:
                print("A cube needs four coordinates: "
                      "its lower corner or centre (3) and "
                      "(half-)side-length (1)")
                set_trace()

            # Set up of lower box corner differs depending on anchor type
            box[:, 0] = coords[:3]
            box[:, 1] = coords[:3] + coords[3]
            if self.anchor == 'centre':
                box[:, 0] -= coords[3]
                
        elif self.shape == "box":
            if len(coords) != 6:
                print("A box needs six coordinates: its lower (3) and "
                      "upper (3) vertices, or centre (3) and half-side "
                      "lengths (3)")
                set_trace()

            if self.anchor == 'centre':
                box[:, 0] = coords[:3] - coords[3:]
                box[:, 1] = coords[:3] + coords[3:]
            elif self.anchor == 'bottom':
                box[:, 0] = coords[:3]
                box[:, 1] = coords[3:]
            else:
                print("Unrecognised anchor choice '{:s}'"
                      .format(self.anchor))
                set_trace()
                
        else:
            print("Your shape '{:s}' is not yet implemented."
                  .format(self.shape))
            set_trace()

        return box

    def _identify_relevant_cells(self, box, f):
        """
        Identify all cells intersecting the selection box.
    
        This function uses the lower corner of the region covered with cells
        and the cell size from the particle maps.

        Parameters
        ----------

        box : array [3, 2]
            The lower ([:, 0]) and upper ([:, 1]) vertices of the selection 
            box as determined by _make_selection_box(). 
        f : h5py file handle
            The handle to the previously opened particle map file.

        Returns
        -------

        cellOffsets : array (int) [3]
            The first cell that intersects the selection box, per dimension.
        cellLengths : array (int) [3]
            The number of cells intersecting the box, per dimension.
            In other words, in dimension i, cells cellOffsets[i] up to and
            including cellOffsets[i]+cellLengths[i]-1 must be checked. 

        Note
        ----

        The function checks whether the selection box extends beyond the edge
        of the mapped region, and clips the box to the edge if so.

        """

        # No particles --> no cells
        header = f["Header"]
        self.numPartTotal = header.attrs["NumPart_Total"][self.pt_num]
        if self.numPartTotal == 0:
            return [0,0,0], [0,0,0]

        # Get the key numbers from the particle map file
        ptGroup = f[self.baseGroup]
        cellCorner = ptGroup.attrs["CellRegionCorner"][:]
        cellSize = ptGroup.attrs["CellSize"][0]
        numCellsPerDim = ptGroup.attrs["NumCellsPerDim"][:]
        cellTop = cellCorner + cellSize*numCellsPerDim
        
        if self.verbose:
            print("cellCorner =", cellCorner)
            print("cellSize =", cellSize)

        # Sanity check to make sure the box does not go below or above
        # the mapped region (also upper/lower box corner!)
        if np.min(box[:, 0] - cellCorner) < 0:
            if not self.silent:
                print("Warning: selection box extends below mapped region.\n"
                      "Clipping it to edge of the map...")
            box[:, 0] = np.clip(box[:, 0], cellCorner, None)
            box[:, 1] = np.clip(box[:, 1], cellCorner, None)            
        if np.min(cellTop - box[:, 1]) < 0:
            if not self.silent:
                print("Warning: selection box extends above mapped region.\n"
                      "Clipping it to edge of the map...")
            box[:, 0] = np.clip(box[:, 0], None, cellTop)
            box[:, 1] = np.clip(box[:, 1], None, cellTop)
            
        # Find first cell to load in each dimension. Note that this is
        # never negative (we just made sure), so would not strictly have
        # to use np.floor here.
        cellOffsets = np.floor((box[:, 0] - cellCorner)/cellSize).astype(int)
        if np.min(cellOffsets) < 0:
            print("Why on Earth do we have negative cellOffsets???")
            set_trace()
        
        # Find number of cells to load in each dimension
        # (need to clip cellEnds to accommodate rounding errors --
        #  *not* needed with cellOffsets because box[:, 0] is clipped
        #  exactly to cellCorner)
        cellEnds = np.ceil((box[:, 1] - cellCorner)/cellSize).astype(int)
        cellEnds = np.clip(cellEnds, None, numCellsPerDim)
        cellLengths = cellEnds - cellOffsets 

        if self.verbose:
            print("cellOffsets =", cellOffsets)
            print("cellLengths =", cellLengths)
        
        return cellOffsets, cellLengths                


    def _find_segments(self, map_file, cell_offsets, cell_lengths):
        """
        Determine the 'segments' (cell sections lying entirely in one file)
        that have to be loaded.
        
        Parameters
        ----------
        map_file : str
            The name of the particle map file 
        cell_offsets : array (int) [3]
            The first cell that intersects the selection box, per dimension.
        cell_lengths : array (int) [3]
            The number of cells intersecting the box, per dimension.

        Returns
        -------
        files : array (int) [N_segments]
            The file index in which each segment is located.
        offsets : array (int) [N_segments]
            The segments' first particle in the respective file.
        lengths : array (int) [N_segments]
            The number of particles in each segment.

        Note:
        -----
        Currently, the file offsets are determined for all cells at the 
        start of the function. This may be inefficient for small cell numbers.
        """

        # Load the map data
        f = h5.File(map_file, 'r')
        ptGroup = f[self.base_group]
        map_cells_per_dim = ptGroup.attrs["NumCellsPerDim"]
        map_cells = ptGroup.attrs["NumCellsTot"][0]
        if self.verbose:
            print("Cells per dimension in map:", map_cells_per_dim)

        # To avoid confusion with the (overall) cellOffsets/-Counts,
        # call the 'particles-in-map-cells' counters 'mapOffsets/-Counts
        map_counts = self._read_hdf5_direct(ptGroup, "CellCount", map_cells)
        map_offsets = self._read_hdf5_direct(ptGroup, "CellOffset", map_cells)
        file_offsets = ptGroup["FileOffset"][:]
        if self.verbose:
            print("file_offsets=", file_offsets)
            
        map_cell_files = np.searchsorted(file_offsets, map_offsets,
                                     side='right')-1
        num_files = len(file_offsets)-1
        
        # Set up output arrays (large enough to account for worst-case of
        # each file gap dividing a selected cell)
        max_segments = self.num_cells + num_files
        files = np.zeros(max_segments, dtype=int)-1
        offsets = np.zeros(max_segments, dtype=int)-1
        lengths = np.zeros(max_segments, dtype=int)

        # Now loop through all possibly relevant cells...
        num_checked = 0
        num_segments = 0
        for cz in range(cell_offsets[2], cell_offsets[2]+cell_lengths[2]):
            for cy in range(cell_offsets[1], cell_offsets[1]+cell_lengths[1]):
                for cx in range(cell_offsets[0],
                                cell_offsets[0]+cell_lengths[0]):
                    # Deal with possibility of periodic wrapping
                    # (assume entire box is mapped in this case!)
                    cxx, cyy, czz = cx, cy, cz
                    if self.periodic:
                        if cxx < 0:
                            cxx += map_cells_per_dim[0]
                        elif cxx >= map_cells_per_dim[0]:
                            cxx -= map_cells_per_dim[0]
                        if cyy < 0:
                            cyy += map_cells_per_dim[1]
                        elif cyy >= map_cells_per_dim[1]:
                            cyy -= map_cells_per_dim[1]
                        if czz < 0:
                            czz += map_cells_per_dim[2]
                        elif czz >= map_cells_per_dim[2]:
                            czz -= map_cells_per_dim[2]
                            
                    # Construct 1D index of cell we're working on
                    index = (cxx + cyy*map_cells_per_dim[0]
                             + czz*map_cells_per_dim[0]*map_cells_per_dim[1])
                    if (num_checked+1 % 10000 == 0) and not self.silent:
                        print("Checking cell {:d} (segments so far: {:d})"
                              .format(num_checked, num_segments))
                    num_checked += 1
                        
                    # Sanity check to make sure index is valid:
                    if index < 0 or index >= map_cells:
                        print("Problem: trying to access invalid cell index "
                              "({:d}, map_cells = {:d}. Please investigate."
                              .format(index, map_cells))
                        set_trace()

                    # Shortcut in case current cell is empty
                    if map_counts[index] == 0:
                        continue

                    # Ok, if we get here there are some particles in the
                    # current cell --> add to output list
                    #
                    # Need file index and file-offset of first cell particle 
                    cell_file = map_cell_files[index]
                    offset_in_file = map_offsets[index]-file_offsets[cell_file]
                    if offset_in_file >= file_offsets[cell_file + 1]:
                        print("Inconsistent file offset detected...")
                        set_trace()

                    first_elem = map_offsets[index]
                    last_elem = map_offsets[index] + map_counts[index] - 1
                        
                    # If this is a multi-file cell, truncate segment to end
                    # of (current, first) file, and deal with it later
                    if (file_offsets[cell_file + 1] <= last_elem):
                        length_in_file = (file_offsets[cell_file + 1]
                                          - first_elem)
                    else:
                        length_in_file = map_counts[index]
                        
                    # Save current segment in next free place of arrays   
                    files[num_segments] = cell_file
                    offsets[num_segments] = offset_in_file
                    lengths[num_segments] = length_in_file
                    num_segments += 1
                    if num_segments > max_segments:
                        print("Have somehow created too many segments...")
                        set_trace()
                    
                    # Now create additional segments for following file(s)
                    # if necessary:
                    while(file_offsets[cell_file + 1] <= last_elem):
                        cell_file += 1

                        # Number of elements depends on whether even now
                        # the cell extends beyond the end of the file
                        if last_elem >= file_offsets[cell_file + 1]:
                            length_in_file = (file_offsets[cell_file + 1]
                                              - file_offsets[cell_file])
                        else:
                            length_in_file = (lastElem
                                              - file_offsets[cell_file]
                                              + 1)

                        files[num_segments] = cell_file
                        offsets[num_segments] = 0   # Always the case here!
                        lengths[num_segments] = length_in_file
                        num_segments += 1
                        if num_segments > max_segments:
                            print("Have somehow created too many segments...")
                            set_trace()

        # Final touches
        self.num_segments = num_segments
        files = files[:num_segments]
        offsets = offsets[:num_segments]
        lengths = lengths[:num_segments]

        if self.verbose:
            print("Checked {:d} cells, found {:d} segments."
                  .format(num_checked, num_segments))

        return files, offsets, lengths                             

    def _read_hdf5_direct(self, cont, dset, num):
        """Convenience function to read 1D int dataset into array"""

        array = np.zeros(num, dtype=int)
        cont[dset].read_direct(array)
        return array

    def _reduce_segments(self, rel_gap = None):
        """Combines adjoining segments to make reading faster"""

        # Sort segments into sequential order
        sorter = np.lexsort((self.offsets, self.files))
        self.files = self.files[sorter]
        self.offsets = self.offsets[sorter]
        self.lengths = self.lengths[sorter]
        
        # Find segments that directly follow on from the one before

        if rel_gap is None:
            ind_join = np.nonzero(
                (self.offsets[1:] == self.offsets[:-1]+self.lengths[:-1]) &
                (self.files[1:] == self.files[:-1]))[0]+1
        else:
            ind_join = np.nonzero(
                ((self.offsets[1:]+self.lengths[1:] - self.offsets[:-1]) /
                 (self.lengths[1:]+self.lengths[:-1]) < rel_gap) &
                (self.files[1:] == self.files[:-1]))[0]+1

            # Need to 'back-extend' to-be-joined segments to the
            # end of the one they are joining
            len_extra = self.offsets[ind_join] - (self.offsets[ind_join-1] +
                                                  self.lengths[ind_join-1])
            self.offsets[ind_join] -= len_extra
            self.lengths[ind_join] += len_extra

        if not self.silent:
            print("Identified {:d} joins..." .format(len(ind_join)))

        # Don't need to do anything if there's nothing to do
        if len(ind_join) == 0:
            return
            
        num_join = len(ind_join)
        ind_to_join = ind_join - 1
        if np.min(ind_to_join) < 0: set_trace()

        num_joined = 0
        for ii in range(1, 1000):
            
            # Find the ones eligible to join now
            join_pow = 2**ii
            sub_pow = 2**(ii-1)

            if self.verbose:
                print("Join iteration {:d}, power {:d}..."
                      .format(ii, join_pow))
                  
            subind_join_now = np.nonzero(ind_join % join_pow == sub_pow)[0]
                        
            # Join them onto their target
            self.lengths[ind_to_join[subind_join_now]] += (
                self.lengths[ind_join[subind_join_now]])
            self.lengths[ind_join[subind_join_now]] = 0

            num_joined += len(subind_join_now)

            if self.verbose:
                print("Have so far completed {:d} joins..."
                      .format(num_joined))
            if num_joined == num_join:
                break
            
            # Update targets that have just joined
            ind_target_joined = np.nonzero(
                ind_to_join % join_pow == sub_pow)[0]
            targets_abs = np.arange(len(self.files), dtype = int)
            targets_abs[ind_join] = ind_to_join
            ind_to_join[ind_target_joined] = targets_abs[
                ind_to_join[ind_target_joined]]

        ind_remain = np.nonzero(self.lengths > 0)[0]
        print("Retained {:d} segments ({:.2f}%...)"
              .format(len(ind_remain), len(ind_remain)/len(self.files)*100))
        self.files = self.files[ind_remain]
        self.offsets = self.offsets[ind_remain]
        self.lengths = self.lengths[ind_remain]
        self.numSegments = len(ind_remain)
