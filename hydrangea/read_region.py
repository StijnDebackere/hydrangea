# Blank file

import numpy as np
import h5py as h5
import time
import os

from functools import reduce
from operator import mul
from astropy.cosmology import Planck13
import astropy.units as u

import hydrangea.crossref as hxr
import hydrangea.hdf5 as hdf5
from hydrangea.split_file import SplitFile
from hydrangea.reader_base import ReaderBase

# TEMPORARY IMPORTS OF OLD PACKAGES
import sim_tools as st # --> get_conv_astro, eagleread 
import yb_utils as yb  # --> read_hdf5(_attribute), create_reverse_list

from pdb import set_trace


# ----------- READ_REGION CLASS -------------------

class ReadRegion(ReaderBase):
    """
    Set up a region for efficient reading of data from snapshot files.
    
    This class can be called with several parameters to allow easy setup
    of commonly encountered selection regions (sphere, cube, box). Internally,
    the particle map generated by MapMaker is then read and processed into a 
    (typically small) number of segments to read in.

    Parameters
    ----------

    filename : string
        The path of the file containing the data to read. If the data is
        spread over multiple files, it should point to the first of them.
    parttype : int
        The (numerical) particle type to read (0=gas, 1=dm, 4=stars, 5=bh)
    coordinates : array (float)
        The coordinates, in code units, that specify the region to read from.
        This depends on the selected shape (below):
            Sphere (default) --> [x_cen, y_cen, z_cen, r] (r = radius)
            Cube --> [x_cen, y_cen, z_cen, a/2] (a = side length)
            Box --> [x_cen, y_cen_z_cen, ax/2, ay/2, az/2] 
                (ax, ay, az = side lengths along x, y, z axes)
        Note that the cube and box coordinates are interpreted differently if 
        the anchor parameter is set to 'bottom' (see below).
                                                            
    shape : string, optional
        The shape of the region to read from. Valid options are 'sphere',
        'cube', or 'box' (capitalized versions also accepted). If None
        (default), a sphere is used.
    anchor : string, optional
        If 'bottom', the 'centre' coordinates for cube and box shapes are
        interpreted instead as the minimum x, y, z coordinates, and the 
        length(s) as full side length. If 'centre' or None (default), they 
        specify the centre instead, as described above. This parameter has no
        effect when a sphere is selected as shape.
    verbose : bool, optional
        If True, more output is printed (default: False)
    silent : bool, optional
        If True, no output is printed (default: False)
    exact : bool, optional
        If True, the positions of particles within the pre-selected region
        are loaded and an exact selection to only the particles within the
        specified target shape is performed. If False (default), all particles
        in the pre-selection region are flagged for loading.
    astro : bool, optional
        Interpret the supplied coordinates in proper Mpc instead of code 
        units (cMpc/h). Default: True.
    map_file : string, optional
        Specifies the location of the particle map file. If None (default),
        this file is assumed to be in the same directory as the particle data.
    periodic : bool, optional
        If True, the simulation is assumed to be periodic and completely
        tiled with particle map cells. Default: False.
    load_full : bool, optional
        If True, the full particle catalogue is read in regardless of the 
        specified region. Default: False.

    Note
    ----

    Setting up very large regions (>~ 20 Mpc) is not very efficient.
    Therefore, when the code detects that > 1M cells would have to be 
    checked and potentially loaded, the region setup is abandoned and the
    entire particle catalogue will be read in. Full reading is also enforced
    when the sub-selection contains more than 40% of the full catalogue.

    """

    def __init__(self, fileName, partType, coordinates, shape=None,
                 anchor=None, verbose=False, silent=False, exact=False,
                 astro=True, mapFile=None, periodic=False, load_full=False,
                 joinThreshold=100, bridgeThreshold=100, bridgeGap=0.1):

        stime = time.clock()

        # Store 'simple' parameters in internal variables
        self.fileName = fileName
        self.pt_num = partType
        self.baseGroup = "PartType" + str(partType)
        self.coordinates = np.array(coordinates).astype(float)
        self.verbose = verbose
        self.silent = silent
        self.exact = exact
        self.load_full = load_full
        self.periodic = periodic
        
        # Deal with different shape inputs
        if shape in [None, 'sphere', 'Sphere']:
            self.shape = 'sphere'
        elif shape in ['Box', 'box']:
            self.shape = 'box'
        elif shape in ['Cube', 'cube']:
            self.shape = 'cube'
        else:
            print("Your shape '{:s}' is not understood. Sorry."
                  .format(shape))
            set_trace()

        if anchor is None:
            self.anchor = 'centre'
        else:
            self.anchor = anchor
            
        # Set up map file, and check whether it exists
        if mapFile is None:
            mapFile = os.path.dirname(fileName) + '/ParticleMap.hdf5'
        if not os.path.exists(mapFile):
            self.load_full = True
            
        # If we need to load everything, we can quit now
        if self.load_full:
            return

        # If input coordinates are in astro units, need to convert to code
        if astro:
            conv_astro = self.get_astro_conv('Coordinates')
            self.coordinates /= conv_astro
                        
        # Do the actual work of finding segments to be loaded
        self._setup_region(mapFile, joinThreshold, bridgeThreshold, bridgeGap)

        if self.load_full:
            return
                
        if not self.silent:
            print("Region setup took {:.3f} sec."
                  .format(time.clock()-stime))
            print("Selection region contains {:d} cells, {:d} segments, "
                  "{:d} particles, {:d} files"
                  .format(self.numCells, self.numSegments, self.numParticles,
                          len(np.unique(self.files))))
        if verbose:
            print("  (selected files:", np.unique(self.files), ")")

        if self.numParticles >= self.numPartTotal*0.75:
            self.load_full = True
            if not self.silent:
                print("Need to load {:.1f}% of total particle catalogue\n"
                      "  ==> faster to load everything instead"
                      .format(self.numParticles/self.numPartTotal*100))
                
        
    # -----------------------------------------------------------------------
        
    def read_data(self, dataSetName, astro=True, verbose=None,
                  return_conv=False, exact=None, fileName=None,
                  pt_name=None, singleFile=False, silent=None):
        """
        Read a specified dataset for a previously set up region.
        
        Parameters
        ----------

        dataSetName : string
            The dataset to read from, including groups where appropriate. 
            The leading 'PartType[x]' must however *not* be included!

        astro : bool, optional 
            Convert values to proper astronomical units (default: True)
        verbose : bool, optional 
            Enable additional log messages (default: class init value)
        return_conv : bool, optional 
            If True, returns a list of [data, conv_astro, aexp], where 
            conv_astro = conversion factor internal --> astro. Default: False
        exact : bool, optional
            Only return data for particles lying in the exact specified 
            selection region. Defaults to the class init value.
        fileName : string, optional 
            Specifies an alternative path to read data from. This is useful 
            for reading data from ancillary catalogues. By default (None), 
            the file name used to set up the class instance is used.
        pt_name : string, optional
            Specifies an alternative particle-type group name. By default 
            (None), 'PartType[x]' is used.
        singleFile : bool, optional
            If True, the data is assumed to reside in a single file.
            This is useful for ancillary catalogues. Default: False.
        silent : bool, optional
            Suppress all output. Default: class init value

        Returns
        -------

        data : array
            The data read for the particles in the selected region.
        conv_astro : float
            Code-to-astronomical conversion factor; only returned if 
            return_conv is True.
        aexp : float
            The expansion factor at the time of the particle output; only
            returned if return_conv is True.
        """                    

        stime = time.time()

        # Set up default values for non-bool optional parameters
        if fileName is None:
            fileName = self.fileName
        if pt_name is None:
            pt_name = self.baseGroup
        if exact is None:
            exact = self.exact
        if silent is None:
            silent = self.silent
        if verbose is None:
            verbose = self.verbose
            
        # Deal with special case of loading the full particle catalogue 
        if self.load_full:
            if singleFile:
                data_full = yb.read_hdf5(fileName, pt_name + '/' + dataSetName)

                if astro or return_conv:
                    aexp = yb.read_hdf5_attribute(fileName, 'Header',
                                                  "ExpansionFactor")
                    conv_astro = st.get_conv_astro(
                        fileName, pt_name + '/' + dataSetName)

                    if astro and np.issubdtype(data_full.dtype, np.floating):
                        data_full *= conv_astro    
                    if return_conv:
                        return [data_full, conv_astro, aexp]
                    else:
                        return data_full
                                        
            else:
                data_full = st.eagleread(
                    fileName, pt_name + '/' + dataSetName, astro = astro)

                if astro and not return_conv:
                    return data_full[0]
                else:  
                    return data_full    # already in right format

        # Deal with pathological case of no data to load:
        if self.numSegments == 0:
            if return_conv:
                return [np.zeros(0, dtype = int), None, None]
            else:
                return np.zeros(0, dtype = int)
                
        # -------------------------------------------------------------
        # Rest is for 'default' situation of reading via segment list.
        # -------------------------------------------------------------

        # Initialize current offset in output array 
        writeOffset = 0     

        if singleFile:
            f = h5.File(fileName, 'r')
        else:
            unique_files = np.unique(self.files)
            rev_files = yb.create_reverse_list(unique_files)
            file_list = []
            for iifile in range(len(unique_files)):
                currFileIndex = unique_files[iifile]
                currFileName = self._swap_file_name(fileName, currFileIndex)
                file_list.append(h5.File(currFileName, 'r'))
            f = file_list[0]
                
        # Set up output before we iterate:
        dSet = f[pt_name + '/' + dataSetName]
        full_shape = list(dSet.shape)      # Cannot assign to tuple ==> list
        full_shape[0] = self.numParticles
        data_full = np.empty(full_shape, dSet.dtype)
            
        # Get astronomical conversion factor
        if astro or return_conv:
            try:
                hscale_exponent = dSet.attrs["h-scale-exponent"]
                ascale_exponent = dSet.attrs["aexp-scale-exponent"]
            except:
                hscale_exponent = 0
                ascale_exponent = 0
                
            header = f["/Header"]
            aexp = header.attrs["ExpansionFactor"]
            h_hubble = header.attrs["HubbleParam"]

            conv_astro = (aexp**ascale_exponent *
                          h_hubble**hscale_exponent)

        if verbose:
            print("Pre-reading '{:s}' took {:.3f} sec."
                  .format(dataSetName, time.time() - stime))
            
        # Read individual segments into correct location of output array 
        for iiseg in range(self.numSegments):
            if verbose:
                print("Segment {:d}, file {:d}"
                      .format(iiseg, self.files[iiseg]), end="")

            if not singleFile:
                unique_index = rev_files[self.files[iiseg]]
                if unique_index < 0:
                    print("Index inconsistency...")
                    set_trace()
                f = file_list[rev_files[self.files[iiseg]]]

            # Need to consider stupid case of no particles of this type
            # being present in this file at all --> data sets do not exist
            # Actually, this should not happen...
            if pt_name + '/' + dataSetName not in f:
                print("Data set does not exist!")
                set_trace()
            
            dSet = f[pt_name + '/' + dataSetName]
                
            # Find out from where and to where we should read
            readOffset = self.offsets[iiseg]
            readEnd = self.offsets[iiseg] + self.lengths[iiseg]
            writeEnd = writeOffset + self.lengths[iiseg]
            if verbose:
                print(" [{:d} --> {:d}]" .format(readOffset, readEnd))
            
            # If we read from a single file, but the (main) particle data
            # are split, need to adjust location to read from in this file
            if singleFile:
                readOffset += self.fileOffsets[self.files[iiseg]]
                readEnd += self.fileOffsets[self.files[iiseg]]

            if readEnd-readOffset != writeEnd-writeOffset:
                print("Inconsistency...")
                set_trace()
                
            # Actually read the data
            # (no appreciable speed difference between following two lines)
            #data_full[writeOffset:writeEnd] = dSet[readOffset:readEnd]
            #print("Reading iiseg={:d}..." .format(iiseg))
            dSet.read_direct(data_full, np.s_[readOffset:readEnd, ...],
                             np.s_[writeOffset:writeEnd, ...])

            # Update write offset
            writeOffset += self.lengths[iiseg]

        if singleFile:
            f.close()
        else:
            for iifile in range(len(unique_files)):
                file_list[iifile].close()
            
        # ------ Done with main bit, final adjustments if needed -----
            
        # Limit selection if 'exact' is set:
        if exact:
            data_full = data_full[self.ind_sel, ...]

        if astro and np.issubdtype(data_full.dtype, np.floating):
            data_full *= conv_astro    

        if not silent:
            print("Reading '{:s}' took {:.3f} sec."
                  .format(dataSetName, time.time() - stime))

        if return_conv:
            return data_full, conv_astro, aexp
        else:
            return data_full                    

    # `````````````````` End of read_data() ''''''''''''''''''''''''''''

    def total_in_region(self, dataSetName, average=False,
                        weightQuant = None, astro = False):
        """
        Convenience function to compute the total or average of 'quantity' 

        Parameters
        ----------

        dataSetName : string
            The (full) name of the data set to process, including potential
            groups that contain it.
        average : bool, optional
            Compute the average of particles instead of the sum (default).
        weightQuant : string, optional
            The (full) name of a data set to use as weights. If None (default),
            no weighting is performed. Supplying a weightQuant implicitly
            also sets average=True.
        astro : bool, optional
            If True, both the target and (potentially) weight quantities are
            converted to astronomical units. Default: False.
        
        Returns
        -------
        
        sum : array
            The sum or average over all particles in the selection region.
        
        """
        
        if self.exact is False:

            print("")
            print("**********************************************************")
            print("********************   WARNING ***************************")
            print("**********************************************************")
            print("")
            print("You have not set the 'exact' switch when establising this "
                  "region. Be aware that the reported total may include a "
                  "contribution from particles outside the target region. "
                  "Proceed with caution...")            
            print("")
            print("**********************************************************")
            print("")
            
        data = self.read_data(dataSetName, astro = astro)

        if weightQuant is None:
            if average: return np.mean(data, axis = 0)
            else: return np.sum(data, axis = 0)
        else:
            weights = self.read_data(weightQuant, astro = astro)
            return np.average(data, weights=weights, axis=0) 
            
    # --------------------------------------------------------------
    # ----------- INTERNAL-ONLY FUNCTIONS BELOW --------------------
    # --------------------------------------------------------------

    def _setup_region(self, mapFile, joinThreshold, bridgeThreshold,
                      bridgeGap):
        """
        Set up the actual reading region, using the specified particle map
        """
        
        f = h5.File(mapFile, 'r')
 
        # Select rectangular region of simulation to be loaded
        box = self._make_selection_box()
        if self.verbose:
            print("Selection box is [({:.3f} --> {:.3f}), "
                  "({:.3f} --> {:.3f}),\n" 
                  "                  ({:.3f} --> {:.3f})]"
                  .format(*box[0, :], *box[1, :], *box[2, :]))

        # Identify cells intersecting the region of interest
        #     'cellOffsets' is the first cell per dimension
        #     'cellLength' is the number of cells per dimension
        cellOffsets, cellLength = self._identify_relevant_cells(box, f)
        self.numCells = reduce(mul, cellLength, 1)
        if not self.silent:
            print("Checking {:d} cells..." .format(self.numCells))  
        
        # No cells --> nothing to do --> done
        if self.numCells == 0:
            self.numSegments = 0
            self.numParticles = 0
            self.files = np.zeros(0, dtype = int)
            return

        # Too many cells --> too much to do --> done
        if self.numCells > 1e6:
            self.load_full = True
            return

        # The heavy lifting: find file index, offset, and length for
        # all potentially relevant segments
        self.files, self.offsets, self.lengths = self._find_segments(
            cellOffsets, cellLength, f)
        f.close()

        # If there are many segments, try to combine them where possible
        if len(self.files) > joinThreshold:
            self._reduce_segments()

            # Second, more aggressive join: bridge small gaps
            # between segments
            if len(self.files) > bridgeThreshold:
                self._reduce_segments(rel_gap = 1+bridgeGap)

        self.numParticles = np.sum(self.lengths)
        self.numParticlesExact = self.numParticles
                    
        if self.exact:
            self._find_exact_region(self)
    
    def _find_exact_region(self):
        """Find particles lying exactly in the selection region"""
            
        # Need to explicitly set 'exact = False' here, because we have not 
        # yet set up exact loading (we are doing it right now!)
        pt_coords = self.read_data("Coordinates", exact = False)

        anchor = self.coordinates[:3]
        relpos = coords - anchor[None, :]  

        if self.shape == 'sphere':
            relrad = np.linalg.norm(relpos, axis=1)
            self.ind_sel = np.nonzero(relrad <= self.coordinates[3])[0]
        elif self.shape == 'cube':
            if self.anchor == 'bottom':
                self.ind_sel = np.nonzero(
                    (np.min(relpos, axis = 1) >= 0) &
                    (np.max(relpos, axis = 1) <= self.coordinates[3]))[0]
            else:
                self.ind_sel = np.nonzero(
                    np.max(np.abs(relpos), axis = 1) <= self.coordinates[3])[0]
                    
        elif self.shape == 'box':
            length = self.coordinates[3:]
            if self.anchor == 'bottom':
                self.ind_sel = np.nonzero(
                    (np.min(relpos, axis = 1) >= 0) &
                    (np.max(relpos - length[None, :]) <= 0))[0]
            else:
                self.ind_sel = np.nonzero(
                    np.max(np.abs(relpos - length[None, :])) <= 0)[0]
        else:
            print("Invalid shape encountered: '{:s}'." .format(self.shape))
            set_trace()
            
        self.numParticlesExact = len(ind_sel)        
        
    def _make_selection_box(self):
        """
        Find the box enclosing the selection region.
        
        For the moment, *all* particles in this region will be loaded. 
        In future, we may do something more fancy that takes the actual 
        selection shape into account (`may' here includes `may not').

        This function returns a 3x2 array of 6 coordinates specifying the box:
        [[x-, y-, z-], [x+, y+, z+]]
        """

        coords = np.array(self.coordinates)
        box = np.zeros((3, 2))
        
        if self.shape == "sphere":
            if len(coords) != 4:
                print("A sphere needs four coordinates: "
                      "its centre (3), and radius (1)")
                set_trace()
            box[:, 0] = coords[:3] - coords[3]
            box[:, 1] = coords[:3] + coords[3]
                   
        elif self.shape == "cube":
            if len(coords) != 4:
                print("A cube needs four coordinates: "
                      "its lower corner or centre (3) and "
                      "(half-)side-length (1)")
                set_trace()

            # Set up of lower box corner differs depending on anchor type
            box[:, 0] = coords[:3]
            box[:, 1] = coords[:3] + coords[3]
            if self.anchor == 'centre':
                box[:, 0] -= coords[3]
                
        elif self.shape == "box":
            if len(coords) != 6:
                print("A box needs six coordinates: its lower (3) and "
                      "upper (3) vertices, or centre (3) and half-side "
                      "lengths (3)")
                set_trace()

            if self.anchor == 'centre':
                box[:, 0] = coords[:3] - coords[3:]
                box[:, 1] = coords[:3] + coords[3:]
            elif self.anchor == 'bottom':
                box[:, 0] = coords[:3]
                box[:, 1] = coords[3:]
            else:
                print("Unrecognised anchor choice '{:s}'"
                      .format(self.anchor))
                set_trace()
                
        else:
            print("Your shape '{:s}' is not yet implemented."
                  .format(self.shape))
            set_trace()

        return box

    def _identify_relevant_cells(self, box, f):
        """
        Identify all cells intersecting the selection box.
    
        This function uses the lower corner of the region covered with cells
        and the cell size from the particle maps.

        Parameters
        ----------

        box : array [3, 2]
            The lower ([:, 0]) and upper ([:, 1]) vertices of the selection 
            box as determined by _make_selection_box(). 
        f : h5py file handle
            The handle to the previously opened particle map file.

        Returns
        -------

        cellOffsets : array (int) [3]
            The first cell that intersects the selection box, per dimension.
        cellLengths : array (int) [3]
            The number of cells intersecting the box, per dimension.
            In other words, in dimension i, cells cellOffsets[i] up to and
            including cellOffsets[i]+cellLengths[i]-1 must be checked. 

        Note
        ----

        The function checks whether the selection box extends beyond the edge
        of the mapped region, and clips the box to the edge if so.

        """

        # No particles --> no cells
        header = f["Header"]
        self.numPartTotal = header.attrs["NumPart_Total"][self.pt_num]
        if self.numPartTotal == 0:
            return [0,0,0], [0,0,0]

        # Get the key numbers from the particle map file
        ptGroup = f[self.baseGroup]
        cellCorner = ptGroup.attrs["CellRegionCorner"][:]
        cellSize = ptGroup.attrs["CellSize"][0]
        numCellsPerDim = ptGroup.attrs["NumCellsPerDim"][:]
        cellTop = cellCorner + cellSize*numCellsPerDim
        
        if self.verbose:
            print("cellCorner =", cellCorner)
            print("cellSize =", cellSize)

        # Sanity check to make sure the box does not go below or above
        # the mapped region (also upper/lower box corner!)
        if np.min(box[:, 0] - cellCorner) < 0:
            if not self.silent:
                print("Warning: selection box extends below mapped region.\n"
                      "Clipping it to edge of the map...")
            box[:, 0] = np.clip(box[:, 0], cellCorner, None)
            box[:, 1] = np.clip(box[:, 1], cellCorner, None)            
        if np.min(cellTop - box[:, 1]) < 0:
            if not self.silent:
                print("Warning: selection box extends above mapped region.\n"
                      "Clipping it to edge of the map...")
            box[:, 0] = np.clip(box[:, 0], None, cellTop)
            box[:, 1] = np.clip(box[:, 1], None, cellTop)
            
        # Find first cell to load in each dimension. Note that this is
        # never negative (we just made sure), so would not strictly have
        # to use np.floor here.
        cellOffsets = np.floor((box[:, 0] - cellCorner)/cellSize).astype(int)
        if np.min(cellOffsets) < 0:
            print("Why on Earth do we have negative cellOffsets???")
            set_trace()
        
        # Find number of cells to load in each dimension
        # (need to clip cellEnds to accommodate rounding errors --
        #  *not* needed with cellOffsets because box[:, 0] is clipped
        #  exactly to cellCorner)
        cellEnds = np.ceil((box[:, 1] - cellCorner)/cellSize).astype(int)
        cellEnds = np.clip(cellEnds, None, numCellsPerDim)
        cellLengths = cellEnds - cellOffsets 

        if self.verbose:
            print("cellOffsets =", cellOffsets)
            print("cellLengths =", cellLengths)
        
        return cellOffsets, cellLengths                


    def _find_segments(self, cellOffsets, cellLengths, f):
        """
        Determine the 'segments' (cell sections lying entirely in one file)
        that have to be loaded.
        
        Parameters
        ----------

        cellOffsets : array (int) [3]
            The first cell that intersects the selection box, per dimension.
        cellLengths : array (int) [3]
            The number of cells intersecting the box, per dimension.
        f : h5py file handle
            The handle to the previously opened particle map file

        Returns
        -------
        
        files : array (int) [N_segments]
            The file index in which each is located.
        offsets : array (int) [N_segments]
            The first particle in the respective file that belongs to each
            segment.
        lengths : array (int) [N_segments]
            The number of particles in each segment.

        Note:
        -----

        Currently, the file offsets are determined for all cells at the 
        start of the function. This may be inefficient for small cell numbers.

        """

        # Load the map data
        ptGroup = f[self.baseGroup]
        numCellsPerDim = ptGroup.attrs["NumCellsPerDim"]
        numCellsTot = ptGroup.attrs["NumCellsTot"][0]
        if self.verbose:
            print("Cells per dimension in map:", numCellsPerDim)

        # To avoid confusion with the (overall) cellOffsets/-Counts,
        # call the 'particles-in-cells' counters 'partOffsets/-Counts
        partCounts  = self._read_hdf5_direct(ptGroup, "CellCount", numCellsTot)
        partOffsets = self._read_hdf5_direct(ptGroup, "CellOffset", numCellsTot)
        fileOffsets = ptGroup["FileOffset"][:]
        if self.verbose:
            print("fileOffsets=", fileOffsets)
            
        cellFiles = np.searchsorted(fileOffsets, partOffsets, side='right')-1
        numFiles = len(fileOffsets)-1
        
        # Set up output arrays (large enough to account for worst-case of
        # each file gap dividing a selected cell)
        files = np.zeros(self.numCells + numFiles, dtype=int)-1
        offsets = np.zeros(self.numCells + numFiles, dtype=int)-1
        lengths = np.zeros(self.numCells + numFiles, dtype=int)

        # Now loop through all possibly relevant cells...
        nChecked = 0
        nSegments = 0
        for cz in range(cellOffsets[2], cellOffsets[2]+cellLengths[2]):
            for cy in range(cellOffsets[1], cellOffsets[1]+cellLengths[1]):
                for cx in range(cellOffsets[0], cellOffsets[0]+cellLengths[0]):

                    # Deal with possibility of periodic wrapping
                    # (assume entire box is mapped in this case!)
                    cxx, cyy, czz = cx, cy, cz
                    if self.periodic:
                        if cxx < 0:
                            cxx += numCellsPerDim[0]
                        elif cxx >= numCellsPerDim[0]:
                            cxx -= numCellsPerDim[0]
                        if cyy < 0:
                            cyy += numCellsPerDim[1]
                        elif cyy >= numCellsPerDim[1]:
                            cyy -= numCellsPerDim[1]
                        if czz < 0:
                            czz += numCellsPerDim[2]
                        elif czz >= numCellsPerDim[2]:
                            czz -= numCellsPerDim[2]

                    # Construct 1D index of cell we're working on
                    index = (cxx + cyy*numCellsPerDim[0]
                             + czz*numCellsPerDim[0]*numCellsPerDim[1])
                    if (nChecked+1 % 10000 == 0) and not self.silent:
                        print("Checking cell {:d} (segments so far: {:d})"
                              .format(nChecked, nSegments))
                    nChecked += 1
                        
                    # Sanity check to make sure index is valid:
                    if index < 0 or index >= numCellsTot:
                        print("Problem: trying to access invalid cell index "
                              "({:d}, numCells = {:d}. Please investigate."
                              .format(index, numCellsTot))
                        set_trace()

                    # Shortcut in case current cell is empty
                    if partCounts[index] == 0:
                        continue

                    # Ok, if we get here there are some particles in the
                    # current cell --> add to output list
                    firstElem = partOffsets[index]
                    lastElem = partOffsets[index] + partCounts[index] - 1

                    # Need file index and file-offset of first cell particle 
                    cellFile = cellFiles[index]
                    offsetInFile = partOffsets[index] - fileOffsets[cellFile]
                    if offsetInFile >= fileOffsets[cellFile+1]:
                        print("Inconsistent file offset detected...")
                        set_trace()
                    
                    # First, assume that the cell fits fully into its file:
                    lengthInFile = partCounts[index]

                    # If this is a multi-file cell, truncate segment to end
                    # of (current, first) file, and deal with it later
                    if (fileOffsets[cellFile+1] <= lastElem):
                        lengthInFile = fileOffsets[cellFile+1] - firstElem

                    # Save current segment in next free place of arrays   
                    files[nSegments] = cellFile
                    offsets[nSegments] = offsetInFile
                    lengths[nSegments] = lengthInFile
                    nSegments += 1
                    if nSegments > self.numCells+numFiles:
                        print("Have somehow created too many segments...")
                        set_trace()
                    
                    # Now create additional segments for following file(s)
                    # if necessary:
                    while(fileOffsets[cellFile+1] <= lastElem):
                        cellFile += 1

                        # Number of elements depends on whether even now
                        # the cell extends beyond the end of the file
                        if lastElem >= fileOffsets[cellFile + 1]:
                            lengthInFile = (fileOffsets[cellFile + 1]
                                            - fileOffsets[cellFile] + 1)
                        else:
                            lengthInFile = lastElem - fileOffsets[cellFile] + 1

                        files[nSegments] = cellFile
                        offsets[nSegments] = 0   # Always the case here!
                        lengths[nSegments] = lengthInFile
                        nSegments += 1
                        if nSegments > self.numCells+numFiles:
                            print("Have somehow created too many segments...")
                            set_trace()

        # Final touches
        self.numSegments = nSegments
        files = files[:nSegments]
        offsets = offsets[:nSegments]
        lengths = lengths[:nSegments]

        if self.verbose:
            print("Checked {:d} cells, found {:d} segments."
                  .format(nChecked, nSegments))

        return files, offsets, lengths                             

    def _read_hdf5_direct(self, cont, dset, num):
        """Convenience function to read 1D int dataset into array"""

        array = np.zeros(num, dtype=int)
        cont[dset].read_direct(array)
        return array

    def _reduce_segments(self, rel_gap = None):
        """Combines adjoining segments to make reading faster"""

        # Sort segments into sequential order
        sorter = np.lexsort((self.offsets, self.files))
        self.files = self.files[sorter]
        self.offsets = self.offsets[sorter]
        self.lengths = self.lengths[sorter]
        
        # Find segments that directly follow on from the one before

        if rel_gap is None:
            ind_join = np.nonzero(
                (self.offsets[1:] == self.offsets[:-1]+self.lengths[:-1]) &
                (self.files[1:] == self.files[:-1]))[0]+1
        else:
            ind_join = np.nonzero(
                ((self.offsets[1:]+self.lengths[1:] - self.offsets[:-1]) /
                 (self.lengths[1:]+self.lengths[:-1]) < rel_gap) &
                (self.files[1:] == self.files[:-1]))[0]+1

            # Need to 'back-extend' to-be-joined segments to the
            # end of the one they are joining
            len_extra = self.offsets[ind_join] - (self.offsets[ind_join-1] +
                                                  self.lengths[ind_join-1])
            self.offsets[ind_join] -= len_extra
            self.lengths[ind_join] += len_extra

        if not self.silent:
            print("Identified {:d} joins..." .format(len(ind_join)))

        # Don't need to do anything if there's nothing to do
        if len(ind_join) == 0:
            return
            
        num_join = len(ind_join)
        ind_to_join = ind_join - 1
        if np.min(ind_to_join) < 0: set_trace()

        num_joined = 0
        for ii in range(1, 1000):
            
            # Find the ones eligible to join now
            join_pow = 2**ii
            sub_pow = 2**(ii-1)

            if self.verbose:
                print("Join iteration {:d}, power {:d}..."
                      .format(ii, join_pow))
                  
            subind_join_now = np.nonzero(ind_join % join_pow == sub_pow)[0]
                        
            # Join them onto their target
            self.lengths[ind_to_join[subind_join_now]] += (
                self.lengths[ind_join[subind_join_now]])
            self.lengths[ind_join[subind_join_now]] = 0

            num_joined += len(subind_join_now)

            if self.verbose:
                print("Have so far completed {:d} joins..."
                      .format(num_joined))
            if num_joined == num_join:
                break
            
            # Update targets that have just joined
            ind_target_joined = np.nonzero(
                ind_to_join % join_pow == sub_pow)[0]
            targets_abs = np.arange(len(self.files), dtype = int)
            targets_abs[ind_join] = ind_to_join
            ind_to_join[ind_target_joined] = targets_abs[
                ind_to_join[ind_target_joined]]

        ind_remain = np.nonzero(self.lengths > 0)[0]
        print("Retained {:d} segments ({:.2f}%...)"
              .format(len(ind_remain), len(ind_remain)/len(self.files)*100))
        self.files = self.files[ind_remain]
        self.offsets = self.offsets[ind_remain]
        self.lengths = self.lengths[ind_remain]
        self.numSegments = len(ind_remain)
